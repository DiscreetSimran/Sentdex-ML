{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "            Adj. Close    HL_PCT  PCT_change  Adj. Volume      label\n",
      "Date                                                                \n",
      "2004-08-19   50.322842  8.441017    0.324968   44659000.0  69.078238\n",
      "2004-08-20   54.322689  8.537313    7.227007   22834300.0  67.839414\n",
      "2004-08-23   54.869377  4.062357   -1.227880   18256100.0  68.912727\n",
      "2004-08-24   52.597363  7.753210   -5.726357   15247300.0  70.668146\n",
      "2004-08-25   53.164113  3.966115    1.183658    9188600.0  71.219849\n"
     ]
    }
   ],
   "source": [
    "# in this module we will pass our features and labels to the classifier we will build \n",
    "# we haven't right now quite defined our features and labels , which we will do in this one\n",
    "\n",
    "# https://www.youtube.com/watch?v=r4mwkS2T9aI&list=PLQVvvaa0QuDfKTOs3Keq_kaG2P55YRn5v&index=4\n",
    "\n",
    "# we are also going to import numpy library because python does not allow using arrays \n",
    "# plus we are going to import sklearn which we will be using for scaling\n",
    "# in terms of scaling , our goal is to keep the features' value b/w -1 to +1\n",
    "# this helps with accuracy and processing speed\n",
    "# there are instances where people choose not to do pre processing .. that will be explained later \n",
    "# we are also using model_selection to create our training and testing samples\n",
    "# it's just a nice way to split the data (it shuffles your data so that you don't have biased data)\n",
    "\n",
    "# we are also bringing in svm\n",
    "# we are not doing svm model right now but regression is also done using svm \n",
    "# so we'll be discussing this as an example here because once we finish regression we won't come back\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import quandl \n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn import preprocessing, model_selection, svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "df=quandl.get('WIKI/GOOGL',api_key='x1sxcyxyfLTV1Ws2tT7y')\n",
    "df=df[['Adj. Open','Adj. High','Adj. Low','Adj. Close','Adj. Volume']]\n",
    "df['HL_PCT']=(df['Adj. High']-df['Adj. Low'])/df['Adj. Low']*100.0\n",
    "df['PCT_change']=(df['Adj. Close']-df['Adj. Open'])/df['Adj. Open']*100.0\n",
    "\n",
    "df=df[['Adj. Close','HL_PCT','PCT_change','Adj. Volume']]\n",
    "\n",
    "forecast_col='Adj. Close'\n",
    "df.fillna(-9999, inplace=True)\n",
    "\n",
    "forecast_out=int(math.ceil(0.01*len(df)))\n",
    "\n",
    "print(forecast_out)\n",
    "\n",
    "df['label']=df[forecast_col].shift(-forecast_out)\n",
    "df.dropna(inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Adj. Close    HL_PCT  PCT_change  Adj. Volume      label\n",
      "Date                                                                \n",
      "2004-08-19   50.322842  8.441017    0.324968   44659000.0  69.078238\n",
      "2004-08-20   54.322689  8.537313    7.227007   22834300.0  67.839414\n",
      "2004-08-23   54.869377  4.062357   -1.227880   18256100.0  68.912727\n",
      "2004-08-24   52.597363  7.753210   -5.726357   15247300.0  70.668146\n",
      "2004-08-25   53.164113  3.966115    1.183658    9188600.0  71.219849\n",
      "[[ 5.03228418e+01  8.44101709e+00  3.24967503e-01  4.46590000e+07]\n",
      " [ 5.43226889e+01  8.53731343e+00  7.22700723e+00  2.28343000e+07]\n",
      " [ 5.48693765e+01  4.06235672e+00 -1.22788010e+00  1.82561000e+07]\n",
      " ...\n",
      " [ 1.18159000e+03  1.54700007e+00  4.76194525e-01  2.77496700e+06]\n",
      " [ 1.11920000e+03  1.81160398e+00 -7.29098295e-01  5.79888000e+06]\n",
      " [ 1.06876000e+03  5.51223574e+00 -2.89384977e+00  3.74246900e+06]]\n",
      "[  69.0782379    67.83941377   68.91272699 ... 1026.55       1054.09\n",
      " 1006.94      ]\n",
      "3389 3389\n"
     ]
    }
   ],
   "source": [
    "# the first thing we need to do is define x and y \n",
    "# generally X is features and y is labels and then we will scale X and then we will redefine x according to forecast_out\n",
    "# and then we will redefine y\n",
    "\n",
    "# drop() will put all the cols of df in X except labels and df.drop() returns a new df & that is converted to an array using np\n",
    "# the integer argument 1 means axis=1 which means coloumn wise dropping \n",
    "\n",
    "# we are scaling all our features together (feature scaling and normalization) before feeding the data to the classifier\n",
    "# but in future if we add some feature to it, it needs to be scaled TOGETHER with all the other data \n",
    "# which means you will have to include it in your training data \n",
    "# this preprocessing.scale() helps in trainig in testing \n",
    "# but it may add to the processing time for projects like high frequency trading where you may skip this step\n",
    "\n",
    "# now we want only those x's for which there is a value of y\n",
    "# and this is so beacuse of shifting (forecast_out)\n",
    "# example :-  a[:-2]   # everything except the last two items \n",
    "# https://stackoverflow.com/questions/45034747/meaning-of-x-x-forecast-out1\n",
    "# https://stackoverflow.com/questions/509211/understanding-slice-notation\n",
    "# beacuse we have used number of days = forecast_out to make our label cloumn and by that much only we have shifted\n",
    "# if forecast_out =10 , that means today we are predicting value of close for the 10 th day from today \n",
    "# and hence we shifted the rows by that number\n",
    "\n",
    "# inplace: It is a boolean which makes the changes in data frame itself if True.\n",
    "# df.dropna(inplace=True)    // Keep the DataFrame with valid entries in the same variable.\n",
    "\n",
    "print(df.head())\n",
    "X=np.array(df.drop(['label'],1))\n",
    "print(X)\n",
    "y=np.array(df['label'])\n",
    "print(y)\n",
    "\n",
    "X=preprocessing.scale(X)\n",
    "\n",
    "# X=X[:-forecast_out+1]\n",
    "# df.dropna(inplace =True)\n",
    "\n",
    "y=np.array(df['label'])\n",
    "print(len(X),len(y))\n",
    "\n",
    "\n",
    "# these are not the correct lengths(len of x should be equal to len of y)\n",
    "# this is caused by the shifting done using the forecasr_out value in the code line :- \"X=X[:-forecast_out+1]\"\n",
    "# we were doing this shifting because we did not want to have rows of X where there wasn't a value of y\n",
    "# but that is not needed because we have used the command :- \"df.dropna(inplace=True)\" in the previous segment of the code \n",
    "# therefore we'll comment out that forecast shiftinf code line\n",
    "# and also the commented command after the forecast_out one is also not needed for the same reason "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9788941692546236\n"
     ]
    }
   ],
   "source": [
    "# now we are ready for creating our training and testing sets\n",
    "# for this we will use cross validation which shuffles the data and splits it\n",
    "# 0.2 means that 20% of the data is for testing \n",
    "# and then we will use X_train and y_train to fit the classifier and the other two for later testing\n",
    "# we will use linear regression classifier and fit the data \n",
    "# and then before using it in real world we will test it and check the accuracy\n",
    "# and we get 97% accuracy when shifted with 1% of the days which is predicting  35 days in advance\n",
    "# but this is linear regression which is based on squared error so this isn't \"accurate\" accurate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_train,X_test,y_train,y_test=model_selection.train_test_split(X,y,test_size=0.2)\n",
    "clf=LinearRegression()\n",
    "clf.fit(X_train,y_train)\n",
    "accuracy=clf.score(X_test,y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7908782434138807\n",
      "0.6653956184480256\n"
     ]
    }
   ],
   "source": [
    "# now we are trying SVM\n",
    "# for this accuracy got lowered -79%\n",
    "# by default the kernel here is linear , if we change the kernel to \"poly\" accuracy decreased further\n",
    "# therefore we are not going to  use SVM in this case\n",
    "\n",
    "\n",
    "\n",
    "clf=svm.SVR()\n",
    "clf.fit(X_train,y_train)\n",
    "accuracy=clf.score(X_test,y_test)\n",
    "print(accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "clf=svm.SVR(kernel='poly')\n",
    "clf.fit(X_train,y_train)\n",
    "accuracy=clf.score(X_test,y_test)\n",
    "print(accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
